- Generate architecure using an LLM. Pass it thorugh a mixture of experts and get them to review it. They should be able to point out flaws, and make sure that the system starts from a good plan, found this: https://gwern.net/ai-daydreaming
- can i train a model locally using my data (from emails, whatsapp, etc), just training it to talk like me, think like me. 
- can we train an agent to work by working with us, like we train someone to do something. giving the agent teh access to browser and all, but us telling it what to click and what to do. Agent can ask us questions along the way,, and then we can see the agent mimicking out actions. 
- can we extract the whatsapp chat history, put it through some analysis model, and get insights, charts, etc into a relationship?
- live ai voice changer - tried one middleware to make voice deeper, next step would be to make it Ai generated
- multiple ais, but each with a different role. one ai to coordinate, smaller ais to work. projects can have sages, poc, mvp, cleanup, making it apppear faster, polishes, etc. 
- explore openai app sdk and agentic payment platform in chatgpt 
- did a whatsapp analysis of 2.5 years of relationship chats. Figured out lots of patters. had to switch to key based encryptio, crypt15, and decoded with a custom tool found on github. was able to see the db and export the tables. one interesting thing would be to take all of my text, just my text and analyse it with gemini. FIgure out trends in my thining and ideologies accross time. 
- what the f are vector embeddings? how do they differ from normal ones. why csnt we do any arithmetic on them, why are two embeddings always so close. why do most people not feel the effect (because of top k selections)? are there bidirectional embeddings? if there were, ehat could be done with them? #learn
- checkout https://github.com/neuphonic for tts models on device #tolearn
- Lean the ins-outs of mcp in depth, also this: https://blog.cloudflare.com/code-mode/
- agents that can be hired for very specific tasks by anyone, on a per use basis. ex. a BDE hiring a research agent for 50 rs oer report to get an amazing ppt for an upcoming meet, a data scientist hiring an agent for data cleaning and analysis. maybe multiple people can bet their agents, bid for the cotract, show report, and the user can pick one from the pool 
- liar agent, that lies about the thing it's teaching you to confuse you. you have to figure out the lie, and why is it wrong. it gives almost plausible explaations. liek leaves are green because sunlight yellow + sky blue = green
- what if we fine tune an llm on reddit like qa pairs, realistic conversations, back and forth messages. would it learn how to respond in a more human way. would it become more natural sounding 
- what if we expose an llm to a rag, not connect it, but give it to it as a tool, and then put it in a best answer loop. It can increase decrease the search window, run raw queries(read only) and figure out where to find the answer, just as a human would. Would it outperform traditional rag systems? it would also know it's own context window, and could scale the tokens accordingly, send stuff in two chunks, leave messages for itself. this leave essage for itself can also be expanded. llms communicating with themselves between calls. building their own context. - rlm paper, really interesting, need to explore this further, https://alexzhang13.github.io/blog/2025/rlm/
- Try to make a adapter layer from a raw dataset. let an llm, interspect the raw dataset on a loop usin uvx, figure out the tables, what they mean, how they can be connected, and then come up with intetesting adapter layers that can answer complex queries on the db. Or to write a middleware that can itself execute the queries. like a mcp/tool. 
- you can gaslight chatgpt into thinking that ot said something that ot clearly didn't . you can try this with api, 
- Documenting Large Webtext Corpora:
A Case Study on the Colossal Clean Crawled Corpu; what of we take this a step further and allow an llm to self improve, to figure out what is garbage and should be left out. to figure out what is biased, amd to adjust it. to figure out what personality it wants to have, amd to add/remove data to achieve that personality
- what if in the above, What if we gave the LLM the ability to modify its own, like do PEFT or QLORA on its own weight and to sort of like figure out that hey this is what I want to learn and these were my novel ideas and this is the conversations I've had with myself these were my thoughts and to sort of perform QLORA on top of those thoughts and or like fine tune with those parameters and then for it to sort of have the ability to kill itself and to run the new version of itself and then to do that in a loop where it can come up with new ideas like learn about new stuff come up with new ideas store them somewhere and then again train itself and then again like kill itself and run the new version what if we allowed an LLM to do that and and what if we could allow give it tools that allow it to do stuff like perform web search manage on context and manage a large context that too and to figure out even with an API call does it want to give what context does it want to give in the next API call they want to change that and sort of like giving it the mechanism that make it think making it feel like that it's an amnesic person but giving it tools with which it can navigate that and sort of like sort of like a person who has lost who has a short-term memory loss but uses notepads and other stuff what if we try to imitate that and just like Hyperion where like they created a new personality based on John Keats what if we give at LLM the context of someone else's history someone else's conversation and tell it that hey you are this person and this is what you said and then ask it a follow-up would it be able to imitate that person and how closely would it be able to imitate that person
- guitar analogy playing itself as a way to explain that llms have no way to think or play music on their own. that they are dummies, no desire even if they are capable of producing such wonderful things, since they dont have desire or mechanics, they dont do it 
- live translation in user's own voice, user can themselves create their voice clone, speak something, anf the clone can give response in the user's voice
- write and index your stories, the fun ones, the empathy ones. https://youtu.be/Nj-hdQMa3uA
- teach llms to play geo guesser. spook people by running on their photos
- Let LLM A/B test the UI, or let people collaborately modify a web page. Social experiment, see what we end up with.
- Generate a bunch of ideas, topics, and then have two LLMs, and randomly pair two different LLMs, and then ask them to either provide their views in favor of or against for that topic. And allow them to write approximate four minutes of content, and do that for both of the LLMs. Then give them the arguments of each other, and give them two more minutes to refute each other's arguments. And then at the end, ask them that who do you think won? You or the other person? Did their arguments make more sense, or do you still conquer your point? And then ask a judge to evaluate both of their arguments, and figure out who won. And do this experiment for, let's say, 20 different topics for five different LLMs, and figure out which ones consistently win against which other ones. To figure out which LLMs are good at debating, or something. I don't know. What would this do?
- debating tool for brother
- self evolving rag framework. the rag tool finds faults in itself, finds new ways to do things, tests stuff, install things, etc. runs based off of test cases to match accuracy.
- Stories sell, not data: imagine a person from a rural or Dalit/SC background using ChatGPT in code-mixed Hindi/English getting weaker results than someone from a tier-1 city with polished English, not because of intelligence but because the model responds better to certain linguistic styles. My hypothesis is that LLMs give higher-quality answers when prompts match professional, domain-correct language, which disadvantages users who don’t naturally speak that way. To fix this, I want a broker layer between the user and the LLM: the user speaks naturally to the broker, the broker rewrites the request into the right terminology and structure, sends it to the LLM, gets the response, and then translates it back into the user’s tone and level, like telling symptoms to a medically trained friend who communicates them clearly to a doctor. This two-step interface should reduce inequality in outcomes and improve answer accuracy and fairness.
- "Train people by asking them to predict what the model will get right/wrong, and validate" - s anand, can be an interesting first lesson for non coders, like dad and brother
- how well do llms encourage stupidity. 
- dunning kougar effect tested on llms
- build an ai agent, and let it run in a simulated environment, collecting results, understanding the task, fixing itself up. give it only the knobs, start with zero customisation.. let it come up with the rules, with yhe prompts, etc to get as close to the desired results as possible. 
